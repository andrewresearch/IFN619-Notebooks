{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3B: Advanced Numerical Techniques and Potential Ethical Concerns (part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Neural Networks are one of the most powerfull techniques in Computer Science. They are computational systems vaguely inspired by the biological neural networks of animal brains.\n",
    "\n",
    "The image shows two neurons connected with each other. In this image, we can identify three important properties:\n",
    "\n",
    "- **Dendrites:** correspond to the input \"wires\" that receive sensory information. Dendrites are the segments of the neuron that receive stimulation in order for the cell to become active. They conduct electrical messages to the neuron cell body for the cell to function\n",
    "\n",
    "- **Axon:** corresponds to the body of the neuron. It is a long slender projection of a nerve cell, or neuron, that conducts electrical impulses away from the neuron's cell body or soma.\n",
    "\n",
    "- **Synapses:** correspond to the output \"wires\". When a nerve impulse reaches the synapse at the end of a neuron, it cannot pass directly to the next one. Instead, it triggers the neuron to release a chemical neurotransmitter. The neurotransmitter drifts across the gap between the two neurons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/neuron.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rosenblatt's Perceptron: the first neuron\n",
    "\n",
    "A neuron is a computational unit that receives a number of **inputs** through the **dendrites**, performs some **computations** and then it sends **outputs** to the **synapses** via the **axon** to the other neurons in the brain. Neurons communicate with each other through some pulses of electricity also called spikes.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*o7aTsRvL1NocOCHqYTKi2g.png\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that data is linearly separable, the goal of the perceptron is to correctly classify the set of patterns into one of two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions in Neural Networks (and Deep Neural Networks)\n",
    "\n",
    "<img src='https://www.pyimagesearch.com/wp-content/uploads/2018/12/keras_conv2d_activation_functions.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember the Machine Learning Process:\n",
    "\n",
    "<img src=\"images/forecast.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ann_visualizer\n",
    "#!pip install sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of the circles dataset with points colored by class\n",
    "from sklearn.datasets import make_circles\n",
    "from numpy import where\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# mlp for the two circles classification problem\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import RandomUniform\n",
    "\n",
    "# Figure Plotting libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary function to plot decision boundaries\n",
    "def plot_decision_boundary(X, y, model, steps=1000, cmap='Paired'):\n",
    "    \"\"\"\n",
    "    Function to plot the decision boundary and data points of a model.\n",
    "    Data points are colored based on their actual label.\n",
    "    \"\"\"\n",
    "    cmap = plt.get_cmap(cmap)\n",
    "    \n",
    "    # Define region of interest by data limits\n",
    "    xmin, xmax = X[:,0].min() - 1, X[:,0].max() + 1\n",
    "    ymin, ymax = X[:,1].min() - 1, X[:,1].max() + 1\n",
    "    steps = 100\n",
    "    x_span = np.linspace(xmin, xmax, steps)\n",
    "    y_span = np.linspace(ymin, ymax, steps)\n",
    "    xx, yy = np.meshgrid(x_span, y_span)\n",
    "\n",
    "    # Make predictions across region of interest\n",
    "    labels = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Plot decision boundary in region of interest\n",
    "    z = labels.reshape(xx.shape)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.contourf(xx, yy, z, cmap=cmap, alpha=0.5)\n",
    "\n",
    "    # Get predicted labels on training data and plot\n",
    "    train_labels = model.predict(X)\n",
    "    ax.scatter(X[:,0], X[:,1], c=y, cmap=cmap, lw=0)\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a random dataset for classification\n",
    "import Symbolic_regression_classification_generator as sm\n",
    "\n",
    "data_gen = sm.gen_classification_symbolic(m='x1**2-x2**2+x1*x2**3', n_samples=1000, flip_y = 0.01)\n",
    "data = pd.DataFrame( data_gen )\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0 = data[ data[2] == 0 ]\n",
    "class_1 = data[ data[2] == 1 ]\n",
    "\n",
    "X = data[[0, 1]]\n",
    "y = data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure( figsize=(8,8) )\n",
    "plt.scatter( class_0[0], class_0[1], color='darkred', marker='x', s=15, label='malignant' )\n",
    "plt.scatter( class_1[0], class_1[1], color='steelblue',  marker='o', s=15, label='benign'  )\n",
    "plt.ylabel('texture_mean', fontsize=12)\n",
    "plt.xlabel('radius_mean', fontsize=12)\n",
    "plt.title('Breast Tumors', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can check the correlation between the variables \n",
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training set and the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "# The perceptron in going to be a sequence of layers. \n",
    "model = Sequential()\n",
    "\n",
    "# We are defining a perceptron with:\n",
    "# 1 units in the input layer (this corresponds to the number of features in our dataset:\n",
    "# we have two features: radius_mean and texture_mean\n",
    "model.add(Dense(1, input_shape=[2], activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "# SGD = Stochastic Gradient Descent\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ann_visualizer.visualize import ann_viz;\n",
    "\n",
    "ann_viz(model, view=True, filename='perceptron', title=\"Frank Rosenblatt's Perceptron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/perceptron.png' width=\"450\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "_, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plot_decision_boundary(X.values,y.values,model, cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "pyplot.plot(history.history['acc'], label='train')\n",
    "pyplot.plot(history.history['val_acc'], label='test')\n",
    "pyplot.ylabel('accuracy', fontsize=12)\n",
    "pyplot.xlabel('iterations', fontsize=12)\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Multilayer Perceptron Model (aka the Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model_2 = Sequential()\n",
    "\n",
    "# we will define a model with 5 units in a hidden layer\n",
    "model_2.add(Dense(5, input_dim=2, activation='tanh'))\n",
    "model_2.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "# SGD = Stochastic Gradient Descent\n",
    "model_2.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise neural network\n",
    "ann_viz(model_2, view=True, filename='neural_network_h1', title=\"Neural Network 1 hidden layer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/neural_network_h1.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "history_2 = model_2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "_, train_acc_2 = model_2.evaluate(X_train, y_train, verbose=0)\n",
    "_, test_acc_2 = model_2.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc_2, test_acc_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plot_decision_boundary(X.values,y.values,model_2, cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "pyplot.plot(history_2.history['acc'], label='train')\n",
    "pyplot.plot(history_2.history['val_acc'], label='test')\n",
    "pyplot.ylabel('accuracy', fontsize=12)\n",
    "pyplot.xlabel('iterations', fontsize=12)\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model_3 = Sequential()\n",
    "\n",
    "# we will define a model with 5 units in a hidden layer\n",
    "model_3.add(Dense(5, input_dim=2, activation='tanh'))\n",
    "model_3.add(Dense(5, activation='tanh'))\n",
    "model_3.add(Dense(5, activation='tanh'))\n",
    "model_3.add(Dense(5, activation='tanh'))\n",
    "model_3.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "# SGD = Stochastic Gradient Descent\n",
    "model_3.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise neural network\n",
    "ann_viz(model_3, view=True, filename='neural_network_h3', title=\"Neural Network 3 hidden layer\")\n",
    "\n",
    "# https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484\n",
    "# https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/\n",
    "# https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "history_3 = model_3.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "_, train_acc_3 = model_3.evaluate(X_train, y_train, verbose=0)\n",
    "_, test_acc_3 = model_3.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc_3, test_acc_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plot_decision_boundary(X.values,y.values,model_3, cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "pyplot.plot(history_3.history['acc'], label='train')\n",
    "pyplot.plot(history_3.history['val_acc'], label='test')\n",
    "pyplot.ylabel('accuracy', fontsize=12)\n",
    "pyplot.xlabel('iterations', fontsize=12)\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many Hidden Layers: problem of neural networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model_many = Sequential()\n",
    "\n",
    "# we will define a model with 5 units in a hidden layer\n",
    "model_many.add(Dense(5, input_dim=2, activation='tanh'))\n",
    "# we will define a model with 3 units in a hidden layer\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(5, activation='tanh'))\n",
    "model_many.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "# SGD = Stochastic Gradient Descent\n",
    "model_many.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "history_many = model_many.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "_, train_acc_many = model_many.evaluate(X_train, y_train, verbose=0)\n",
    "_, test_acc_many = model_many.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc_many, test_acc_many))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plot_decision_boundary(X.values,y.values,model_many, cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "pyplot.plot(history_many.history['acc'], label='train')\n",
    "pyplot.plot(history_many.history['val_acc'], label='test')\n",
    "pyplot.ylabel('accuracy', fontsize=12)\n",
    "pyplot.xlabel('iterations', fontsize=12)\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Network with ReLu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLu = rectified linear activation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model_deep = Sequential()\n",
    "\n",
    "# we will define a model with 5 units in a hidden layer\n",
    "model_deep.add(Dense(10, input_dim=2, activation='relu'))\n",
    "# we will define a model with 3 units in a hidden layer\n",
    "model_deep.add(Dense(9, activation='relu'))\n",
    "model_deep.add(Dense(8, activation='relu'))\n",
    "model_deep.add(Dense(7, activation='relu'))\n",
    "model_deep.add(Dense(5, activation='relu'))\n",
    "model_deep.add(Dense(3, activation='relu'))\n",
    "model_deep.add(Dense(2, activation='relu'))\n",
    "model_deep.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "# SGD = Stochastic Gradient Descent\n",
    "model_deep.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise neural network\n",
    "ann_viz(model_deep, view=True, filename='neural_network_deep', title=\"Deep Neural Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/neural_network_deep.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "history_deep = model_deep.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "_, train_acc_deep = model_deep.evaluate(X_train, y_train, verbose=0)\n",
    "_, test_acc_deep = model_deep.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc_deep, test_acc_deep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plot_decision_boundary(X.values,y.values,model_deep, cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "pyplot.plot(history_deep.history['acc'], label='train')\n",
    "pyplot.plot(history_deep.history['val_acc'], label='test')\n",
    "pyplot.ylabel('accuracy', fontsize=12)\n",
    "pyplot.xlabel('iterations', fontsize=12)\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it yourself!\n",
    "\n",
    "In workshop 2B, you had to classify a breast tumour dataset using Naive Bayes algorithm. In this task, you will do classify again the dataset, but using:\n",
    "\n",
    "- (1) a neural network with the 'tanh' as the activation function (you can choose as many hidden layers as you want)\n",
    "- (2) a deep neural network with the 'relu' activation function (you can choose as many hidden layers as you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset data/breast_data_full.csv\n",
    "# YOUR CODE HERE\n",
    "data = \n",
    "\n",
    "# separate your dataset: \n",
    "# put the variable that you wish to classify (or predict) in one variable\n",
    "# put your sources of evidence (or your features) in another variable\n",
    "# YOUR CODE HERE\n",
    "X = \n",
    "y = \n",
    "\n",
    "# separate the dataset into test set and train set\n",
    "# YOUR CODE HERE:\n",
    "X_train, X_test, y_train, y_test = \n",
    "\n",
    "# define the Machine Learning Model: Neural Network\n",
    "# YOUR CODE HERE:\n",
    "model_tum = \n",
    "\n",
    "# add the required layers\n",
    "# YOUR CODE HERE:\n",
    "\n",
    "\n",
    "# compile the network using: \n",
    "# the 'mean_squared_error' as a loss function\n",
    "# the stochastic gradient descent ('sgd') as the optimization function\n",
    "# and the 'accuracy' as evaluation metric\n",
    "# YOUR CODE HERE:\n",
    "\n",
    "\n",
    "# fit the model to the data -> learning the model\n",
    "# YOUR CODE HERE:\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "\n",
    "\n",
    "# plot the accuracy througout the training process\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
